{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cfa1fbe-689f-4b30-ae13-ab98aa5ae5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "390bfa48-e4f9-40d3-b85b-093092538922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97222d7a-e0d1-41ba-ad50-0c34c072cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, new_size):\n",
    "    img = Image.open(path).convert(mode='RGB')\n",
    "    if new_size:\n",
    "        # for fixed-size squared resizing, leave only the following line uncommented in this if statement\n",
    "        # img = transforms.Resize(img, (new_size, new_size), PIL.Image.BICUBIC)\n",
    "        # img = transforms.Resize((new_size, new_size), interpolation=Image.BICUBIC)(img)\n",
    "        width, height = img.size\n",
    "        max_dim_ix = np.argmax(img.size)\n",
    "        if max_dim_ix == 0:\n",
    "            new_shape = (int(new_size * (height / width)), new_size)\n",
    "            img = transforms.Resize(new_shape, interpolation=Image.BICUBIC)(img)\n",
    "        else:\n",
    "            new_shape = (new_size, int(new_size * (width / height)))\n",
    "            img = transforms.Resize(new_shape, interpolation=Image.BICUBIC)(img)\n",
    "    return transforms.ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a774b592-5a6a-4ddb-a1cd-56d2ef972c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea9658b6-68f7-4824-806b-81914bbaa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contentImgPath = r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\Content Images\\tubingen.jpg\"\n",
    "styleImgPath = r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\Style Images\\023.jpg\"\n",
    "fineSize = 512\n",
    "# image_list = [x for x in listdir(contentPath) if is_image_file(x)]\n",
    "# prep = transforms.Compose([\n",
    "#     transforms.Resize((fineSize, fineSize)),  # Use Resize instead of Scale\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "alpha = 0.2\n",
    "contentImg = load_img(contentImgPath,fineSize)\n",
    "styleImg = load_img(styleImgPath,fineSize)\n",
    "\n",
    "# w,h = contentImg.size\n",
    "\n",
    "# if(w > h):\n",
    "#     if(w != fineSize):\n",
    "#         neww = fineSize\n",
    "#         newh = int(h*neww/w)\n",
    "#         contentImg = contentImg.resize((neww,newh))\n",
    "        # styleImg = styleImg.resize((neww,newh))\n",
    "# else:\n",
    "#     if(h != fineSize):\n",
    "#         newh = fineSize\n",
    "#         neww = int(w*newh/h)\n",
    "#         contentImg = contentImg.resize((neww,newh))\n",
    "#         styleImg = styleImg.resize((neww,newh))\n",
    "\n",
    "# contentImg = prep(contentImg)\n",
    "# styleImg = prep(styleImg)\n",
    "\n",
    "\n",
    "# contentImg = transforms.ToTensor()(contentImg)\n",
    "# styleImg = transforms.ToTensor()(styleImg)\n",
    "# contentImg.squeeze(0),styleImg.squeeze(0)\n",
    "# contentImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fa57c68-130b-46f2-bea3-5f8a0e5c8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_invertor_conv1_1 = nn.Sequential( # Sequential,\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,3,(3, 3)),\n",
    ")\n",
    "vgg_normalised_conv1_1 = nn.Sequential( # Sequential,\n",
    "\tnn.Conv2d(3,3,(1, 1)),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(3,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    ")\n",
    "feature_invertor_conv2_1 = nn.Sequential( # Sequential,\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,3,(3, 3)),\n",
    ")\n",
    "vgg_normalised_conv2_1 = nn.Sequential( # Sequential,\n",
    "\tnn.Conv2d(3,3,(1, 1)),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(3,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    ")\n",
    "feature_invertor_conv3_1 = nn.Sequential( # Sequential,\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,3,(3, 3)),\n",
    ")\n",
    "vgg_normalised_conv3_1 = nn.Sequential( # Sequential,\n",
    "\tnn.Conv2d(3,3,(1, 1)),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(3,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    ")\n",
    "feature_invertor_conv4_1 = nn.Sequential( # Sequential,\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,3,(3, 3)),\n",
    ")\n",
    "vgg_normalised_conv4_1 = nn.Sequential( # Sequential,\n",
    "\tnn.Conv2d(3,3,(1, 1)),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(3,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    ")\n",
    "feature_invertor_conv5_1 = nn.Sequential( # Sequential,\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,3,(3, 3)),\n",
    ")\n",
    "vgg_normalised_conv5_1 = nn.Sequential( # Sequential,\n",
    "\tnn.Conv2d(3,3,(1, 1)),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(3,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,64,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(64,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,128,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(128,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,256,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(256,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    "\tnn.MaxPool2d((2, 2),(2, 2),(0, 0),ceil_mode=True),\n",
    "\tnn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "\tnn.Conv2d(512,512,(3, 3)),\n",
    "\tnn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6312f1d-b6cb-443f-b530-f1c866a2fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import torchfile\n",
    "# from torch.utils.serialization import load_lua\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ce3af4c-3b9b-4857-bd16-a538af8fa87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, depth):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        assert(type(depth).__name__ == 'int' and 1 <= depth <= 5)\n",
    "        self.depth = depth\n",
    "\n",
    "        if depth == 1:\n",
    "            self.model = vgg_normalised_conv1_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\vgg_normalised_conv1_1.pth\"))\n",
    "        elif depth == 2:\n",
    "            self.model = vgg_normalised_conv2_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\vgg_normalised_conv2_1.pth\"))\n",
    "        elif depth == 3:\n",
    "            self.model = vgg_normalised_conv3_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\vgg_normalised_conv3_1.pth\"))\n",
    "        elif depth == 4:\n",
    "            self.model = vgg_normalised_conv4_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\vgg_normalised_conv4_1.pth\"))\n",
    "        elif depth == 5:\n",
    "            self.model = vgg_normalised_conv5_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\vgg_normalised_conv5_1.pth\"))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, depth):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        assert (type(depth).__name__ == 'int' and 1 <= depth <= 5)\n",
    "        self.depth = depth\n",
    "\n",
    "        if depth == 1:\n",
    "            self.model = feature_invertor_conv1_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\feature_invertor_conv1_1.pth\"))\n",
    "        elif depth == 2:\n",
    "            self.model = feature_invertor_conv2_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\feature_invertor_conv2_1.pth\"))\n",
    "        elif depth == 3:\n",
    "            self.model = feature_invertor_conv3_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\feature_invertor_conv3_1.pth\"))\n",
    "        elif depth == 4:\n",
    "            self.model = feature_invertor_conv4_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\feature_invertor_conv4_1.pth\"))\n",
    "        elif depth == 5:\n",
    "            self.model = feature_invertor_conv5_1\n",
    "            self.model.load_state_dict(torch.load(r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\encoders and decoders\\models\\feature_invertor_conv5_1.pth\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "637838ae-0df2-4e41-9167-bdd8ba6ffc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Encoder(1)\n",
    "e2 = Encoder(2)\n",
    "e3 = Encoder(3)\n",
    "e4 = Encoder(4)\n",
    "e5 = Encoder(5)\n",
    "\n",
    "d1 = Decoder(1)\n",
    "d2 = Decoder(2)\n",
    "d3 = Decoder(3)\n",
    "d4 = Decoder(4)\n",
    "d5 = Decoder(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e44a81d-2e9d-4f90-8259-c1e28e4d33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " def whiten_and_color(cF,sF):\n",
    "        cFSize = cF.size()\n",
    "        c_mean = torch.mean(cF,1) # c x (h x w)\n",
    "        c_mean = c_mean.unsqueeze(1).expand_as(cF)\n",
    "        cF = cF - c_mean\n",
    "\n",
    "        contentConv = torch.mm(cF,cF.t()).div(cFSize[1]-1) + torch.eye(cFSize[0]).double()\n",
    "        c_u,c_e,c_v = torch.svd(contentConv,some=False)\n",
    "\n",
    "        k_c = cFSize[0]\n",
    "        for i in range(cFSize[0]):\n",
    "            if c_e[i] < 0.00001:\n",
    "                k_c = i\n",
    "                break\n",
    "\n",
    "        sFSize = sF.size()\n",
    "        s_mean = torch.mean(sF,1)\n",
    "        sF = sF - s_mean.unsqueeze(1).expand_as(sF)\n",
    "        styleConv = torch.mm(sF,sF.t()).div(sFSize[1]-1)\n",
    "        s_u,s_e,s_v = torch.svd(styleConv,some=False)\n",
    "\n",
    "        k_s = sFSize[0]\n",
    "        for i in range(sFSize[0]):\n",
    "            if s_e[i] < 0.00001:\n",
    "                k_s = i\n",
    "                break\n",
    "\n",
    "        c_d = (c_e[0:k_c]).pow(-0.5)\n",
    "        step1 = torch.mm(c_v[:,0:k_c],torch.diag(c_d))\n",
    "        step2 = torch.mm(step1,(c_v[:,0:k_c].t()))\n",
    "        whiten_cF = torch.mm(step2,cF)\n",
    "\n",
    "        s_d = (s_e[0:k_s]).pow(0.5)\n",
    "        targetFeature = torch.mm(torch.mm(torch.mm(s_v[:,0:k_s],torch.diag(s_d)),(s_v[:,0:k_s].t())),whiten_cF)\n",
    "        targetFeature = targetFeature + s_mean.unsqueeze(1).expand_as(targetFeature)\n",
    "        return targetFeature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99d47f23-701f-4d89-a8d7-16a42396c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(cF,sF,csF,alpha):\n",
    "        cF = cF.double()\n",
    "        sF = sF.double()\n",
    "        C,W,H = cF.size(0),cF.size(1),cF.size(2)\n",
    "        _,W1,H1 = sF.size(0),sF.size(1),sF.size(2)\n",
    "        cFView = cF.view(C,-1)\n",
    "        sFView = sF.view(C,-1)\n",
    "\n",
    "        targetFeature = whiten_and_color(cFView,sFView)\n",
    "        targetFeature = targetFeature.view_as(cF)\n",
    "        ccsF = alpha * targetFeature + (1.0 - alpha) * cF\n",
    "        ccsF = ccsF.float().unsqueeze(0)\n",
    "        # print(\"ccsF:\",ccsF.size())\n",
    "        if csF.size(0) == 0:\n",
    "            csF = torch.empty_like(ccsF)\n",
    "        csF.data.resize_(ccsF.size()).copy_(ccsF)\n",
    "        # print(\"csF:\",csF.shape)\n",
    "        return csF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b64f599a-565e-4996-a457-9834d2f72ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import scipy.misc\n",
    "# from torch.utils.serialization import load_lua\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe8173cb-6df8-4bbf-ad70-49b362bae012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu(x):\n",
    "        return torch.sum(x,(2,3))/(x.shape[2]*x.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "707115f4-6103-4a7a-bce3-ff32256613ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "        return torch.sqrt((torch.sum((x.permute([2,3,0,1])-mu(x)).permute([2,3,0,1])**2,(2,3))+0.000000023)/(x.shape[2]*x.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "418ab930-e5c6-4d59-92f0-fcefd6bb4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y):\n",
    "        return (sigma(y)*((x.permute([2,3,0,1])-mu(x))/sigma(x)) + mu(y)).permute([2,3,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3704871-b49d-4fcc-9026-3954ca642dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def styleTransfer(contentImg,styleImg,csF):\n",
    "\n",
    "    sF5 = e5(styleImg)\n",
    "    cF5 = e5(contentImg)\n",
    "    # print(cF5.shape)\n",
    "    sF5 = sF5.data.cpu().squeeze(0)\n",
    "    cF5 = cF5.data.cpu().squeeze(0)\n",
    "    # csF5 = transform(cF5,sF5,csF,alpha)\n",
    "    cF5=cF5.unsqueeze(0)\n",
    "    sF5=sF5.unsqueeze(0)\n",
    "    csF5 = forward(cF5,sF5)\n",
    "    # csF5 = csF5.data.cpu().squeeze(0)\n",
    "    Im5 = d5(csF5)\n",
    "\n",
    "    sF4 = e4(styleImg)\n",
    "    cF4 = e4(Im5)\n",
    "    sF4 = sF4.data.cpu().squeeze(0)\n",
    "    cF4 = cF4.data.cpu().squeeze(0)\n",
    "    cF4=cF4.unsqueeze(0)\n",
    "    sF4=sF4.unsqueeze(0)\n",
    "    # csF4 = transform(cF4,sF4,csF,alpha)\n",
    "    csF4 = forward(cF4,sF4)\n",
    "    # csF4 = csF4.data.cpu().squeeze(0)\n",
    "    Im4 = d4(csF4)\n",
    "    print(Im4.shape)\n",
    "\n",
    "    sF3 = e3(styleImg)\n",
    "    cF3 = e3(Im4)\n",
    "    sF3 = sF3.data.cpu().squeeze(0)\n",
    "    cF3 = cF3.data.cpu().squeeze(0)\n",
    "    cF3=cF3.unsqueeze(0)\n",
    "    sF3=sF3.unsqueeze(0)\n",
    "    # csF3 = transform(cF3,sF3,csF,alpha)\n",
    "    csF3 = forward(cF3,sF3)\n",
    "    # csF3 = csF3.data.cpu().squeeze(0)\n",
    "    Im3 = d3(csF3)\n",
    "\n",
    "    sF2 = e2(styleImg)\n",
    "    cF2 = e2(Im3)\n",
    "    sF2 = sF2.data.cpu().squeeze(0)\n",
    "    cF2 = cF2.data.cpu().squeeze(0)\n",
    "    cF2=cF2.unsqueeze(0)\n",
    "    sF2=sF2.unsqueeze(0)\n",
    "    # csF2 = transform(cF2,sF2,csF,alpha)\n",
    "    csF2 = forward(cF2,sF2)\n",
    "    # csF2 = csF2.data.cpu().squeeze(0)\n",
    "    Im2 = d2(csF2)\n",
    "\n",
    "    sF1 = e1(styleImg)\n",
    "    cF1 = e1(Im2)\n",
    "    sF1 = sF1.data.cpu().squeeze(0)\n",
    "    cF1 = cF1.data.cpu().squeeze(0)\n",
    "    cF1=cF1.unsqueeze(0)\n",
    "    sF1=sF1.unsqueeze(0)\n",
    "    # csF1 = transform(cF1,sF1,csF,alpha)\n",
    "    csF1 = forward(cF1,sF1)\n",
    "    # csF1 = csF1.data.cpu().squeeze(0)\n",
    "    Im1 = d1(csF1)\n",
    "\n",
    "    save_path=r\"C:\\Users\\Jaya Teja\\Universal-Style-Transfer\\Output Images\\only-ADA-IN.png\"\n",
    "    \n",
    "    vutils.save_image(Im1.data.cpu().float(),save_path)\n",
    "    # image_np = Im1.data.cpu().float().numpy()\n",
    "\n",
    "    # plt.imshow(image_np.transpose(1, 2, 0))  # Transpose to (H, W, C) for displaying RGB image\n",
    "    # plt.axis('off')  # Hide axis ticks and labels\n",
    "    # plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "daa92ce7-e424-40ce-89fb-2b474f289b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 512])\n",
      "Elapsed time is: 15.855047\n"
     ]
    }
   ],
   "source": [
    "avgTime = 0\n",
    "cImg = torch.Tensor()\n",
    "sImg = torch.Tensor()\n",
    "csF = torch.Tensor()\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    cImg = Variable(contentImg)\n",
    "    sImg = Variable(styleImg)\n",
    "    csF = Variable(csF)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "styleTransfer(cImg,sImg,csF)\n",
    "end_time = time.time()\n",
    "print('Elapsed time is: %f' % (end_time - start_time))\n",
    "avgTime += (end_time - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9a56b-2347-454c-a57a-3d6c99a84689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c24cb-0e9d-40bc-950f-966e6a6dc11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bd271-1d56-42c8-a0cc-3b44512f398a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a53fd-b918-4487-ab95-6c8899be7c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
